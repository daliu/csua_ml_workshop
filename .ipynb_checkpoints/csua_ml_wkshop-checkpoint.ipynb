{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>CSUA ML Workshop</h1>\n",
    "<p>Welcome to the CSUA ML Workshop. This is intended to be for beginners with little to no experience in machine learning or artificial intelligence, and is meant to not only teach the implementation of ML but also provide some of the motivating factors behind it.</p>\n",
    "\n",
    "<p>TensorFlow is required for this workshop due to its beginner friendliness. If you are looking for a more customizable framework, it is best to use Torch. A rundown of the two is below.</p>\n",
    "\n",
    "<h2>TensorFlow</h2>\n",
    "<p>One of the hottest Machine Learning frameworks, TensorFlow is managed by Google and is used internally by them as well. </p>\n",
    "<h3>Pros</h3>\n",
    "- Google \n",
    "- Well abstracted and high-level\n",
    "- More built-in models to play with\n",
    "- Quicker to get off the ground\n",
    "- Set, rigid models for optimization\n",
    "\n",
    "<h3>Cons</h3>\n",
    "- Google \n",
    "- Still slower than Torch\n",
    "- Autodifferentiation is more limited\n",
    "- Writing custom models is a pain in the ass\n",
    "- Customization of models is extremely verbose and tedious\n",
    "- Bloated API\n",
    "\n",
    "<h2>Torch</h2>\n",
    "<p> Also one of the hottest Machine Learning frameworks, but worked on by Facebook, Nvidia, Salesforce, Stanford, etc.</p>\n",
    "<h3>Pros</h3>\n",
    "- Fast\n",
    "- Dynamic models allowing for variable inputs\n",
    "- Better autodifferentiation\n",
    "- Extremely detailed customization available\n",
    "- Relatively sleek API\n",
    "\n",
    "<h3>Cons</h3>\n",
    "- More difficult to get used to\n",
    "- Requires a deeper understanding of ML\n",
    "- Not ideal for building fast\n",
    "- More headaches while creating simpler models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Getting Started</h1>\n",
    "<p>To get started, we will import the libraries that we will use. In this workshop, we will be using TensorFlow, Numpy, and MatPlotLib. TensorFlow and Numpy will be used for numerical manipulation, while MatPlotLib will be used for visualizations. There is also a provided housing_data and housing_test_data. Our first motivating example will be predicting housing prices given square footage.</p>\n",
    "\n",
    "<h2>Motivating Example: Housing Prices</h2>\n",
    "<p>You're looking for houses in the Seattle region and notice that housing prices seem to be very closely correlated with square footage of the house. Can you develop a linear model that predicts housing prices based off of only square footage?</p>\n",
    "\n",
    "<p>The data is provided as a two-dimensional array of [num_examples] pairs. The first value in the pair is the square footage, and the second value is the actual price. <br><br> ```Example: [[1, 300], [2, 600], ...]```</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Import key libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#Import data source \n",
    "import datasets\n",
    "\n",
    "#Generate training and test data. This is a two-dimensional array of lengths [number_of examples, 2]\n",
    "housing_data = datasets.housing_data_gen()\n",
    "housing_test_data = datasets.housing_data_gen()\n",
    "\n",
    "#Prepare data for graphing\n",
    "graph_data = list(zip(*housing_data))\n",
    "\n",
    "plt.scatter(graph_data[0], graph_data[1])\n",
    "plt.xlabel('Square Footage')\n",
    "plt.ylabel('Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Regression</h1>\n",
    "<p>Linear regression is one of those ideas you see everywhere. It is critical to our daily lives and allows humans to extract good patterns from data. Here, we will show how linear regression can be implemented within the context of Machine Learning.</p>\n",
    "\n",
    "<h1>Concept 1: Variables</h1>\n",
    "<p>Variables are the trainable matrices available in TensorFlow. They represent the parameters your model will ultimately be using to predict housing prices. For example, in the equation y = ax + b, a and b would be the parameters associated with the model. Within the context of ML, the variable names w and b would typically be used instead: y = wx + b. The w stands for weight, which makes more sense in the context of neural networks. b stands for bias.</p>\n",
    "\n",
    "<p>In TensorFlow, you can create a variable using: <br><br>\n",
    "```tf.get_variable(name, shape=None, dtype=None, initializer=None, regularizer=None, trainable=True, collections=None, caching_device=None, partitioner=None, validate_shape=True, custom_getter=None)``` <br><br>\n",
    "The only relevant fields for now will be name, shape, and dtype. The name is user-defined. The shape is a list of integers describing the dimensions of the Tensor this variable represents. The dtype is the data type of the Tensor. Note that giving the first dimension \"None\" allows for batch training.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#FILL IN CODE; CREATE WEIGHTS AND BIASES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Concept 2: Placeholders</h1>\n",
    "<p>Ironically, TensorFlow doesn't recommend use of their Tensor type. Placeholders and Variables are used for inputs. They define an operation in the computational graph that represents an input. During actual runtime of the graph, these will have Tensor information flow through it.</p>\n",
    "<p>In TensorFlow, you can create a placeholder using:<br><br>\n",
    "```tf.placeholder(dtype, shape=None, name=None)```<br><br></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#FILL IN CODE; CREATE PLACEHOLDER FOR INPUTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Concept 3: Computational Graphs</h1>\n",
    "<p>A computational graph is what TensorFlow uses to train the model you define. They are very well abstracted, so you can generate the computation graph simply by using matrix operations as you normally would. You can think of the computational graph as a function definition - here, we're merely defining the function. The actual usage will come later.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#FILL IN CODE; BEGIN TO CREATE COMPUTATIONAL GRAPH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Concept 4: Error/Loss Functions</h1>\n",
    "<p>In order for the Linear Regression model to be trained, there must be a way for the model to understand how well it is doing. Because we know what the inputs and outputs are, we would like to define how close the actual prediction is to the actual prediction. The typical loss function for this is Mean Squared Error (MSE) loss.</p>\n",
    "<p>Notice that here, by defining the loss, we're extending the computational graph.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Define a placeholder for the known actual price\n",
    "#FILL IN CODE; PUT IN PLACEHOLDER FOR ACTUAL PRICE\n",
    "\n",
    "#CALCULATE THE MEAN SQUARED ERROR LOSS\n",
    "loss = None #REPLACE LINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>A Note On Data</h1>\n",
    "<p>Please normalize your data before running regressions. While this does obfuscate the actual weights and biases and prevents you from using them in other models, they are easily recoverable with a bit of data manipulation.</p>\n",
    "\n",
    "<p>While this normalization is provided for you, please take a moment to understand what is going on below. A more statistically rigorous way of normalizing data is using the standard deviation, but here I decided to simply divide by the mean to make the code more concise, readable, and clean.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "housing_mean_sqft = sum(graph_data[0]) / len(graph_data[0])\n",
    "housing_sqft_range = max(graph_data[0]) - min(graph_data[0])\n",
    "housing_mean_price = sum(graph_data[1]) / len(graph_data[1])\n",
    "housing_price_range = max(graph_data[1]) - min(graph_data[1])\n",
    "\n",
    "#Normalize data to a -0.5 to 0.5 range by taking (value - mean)/range\n",
    "norm_housing_data = np.array([[(pair[0] - housing_mean_sqft) / housing_sqft_range, \n",
    "                     (pair[1] - housing_mean_price) / housing_price_range] for pair in housing_data])\n",
    "norm_housing_test_data = np.array([[(pair[0] - housing_mean_sqft) / housing_sqft_range, \n",
    "                          (pair[1] - housing_mean_price) / housing_price_range] for pair in housing_test_data])\n",
    "\n",
    "#Randomly shuffle the data to make training more robust\n",
    "np.random.shuffle(norm_housing_data)\n",
    "np.random.shuffle(norm_housing_test_data)\n",
    "\n",
    "#Function to return data back into the i/o space\n",
    "def return_to_io_space(sqft, price):\n",
    "    return sqft * housing_sqft_range + housing_mean_sqft, price * housing_price_range + housing_mean_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Concept 5: Training Operations</h1>\n",
    "<p>To actually train the graph, Tensorflow requires a bit of handling. For TensorFlow, you define an operation on your loss function. The autodifferentiation of TensorFlow will then build up a gradient graph and use it during training. In Torch, the process is slightly more complex. There are a few function calls you must make, one for feedforward, one for calculating gradients, and one for backpropagation. This, however, allows for some slightly more complex and modular training operations.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Concept 6: TensorFlow Runtime</h1>\n",
    "<p>When it's time to train your model, TensorFlow has a few special prerequesites that you need to add to your code to make it run smoothly. Firstly, TensorFlow relies on an idea of a Session - an object acting as a Singleton for all of the computations and runtime variables for your model. Much like how you start a new Python session when running a script, you need to start a new TensorFlow session to run the computational graph.</p>\n",
    "<p>To begin a session, it is best practice to do the following:</p>\n",
    "<p>```with tf.Session() as sess:```</p>\n",
    "<p>After initializing a session, you must initialize all of the Variables that you had previously defined. While before, you were building up a computational graph, now you are initializing the computational graph so it can begin training.</p>\n",
    "<p>```sess.run(tf.global_variables_initializer())```</p>\n",
    "<p>Finally, you can begin training. During training, you can initialize the state of any Variable or Placeholder to some tensor, and the computational graph will automatically use those values in its calculations. You can pass those in using the argument feed_dict, which is a dictionary mapping Placeholders/Variables to arrays/numpy arrays. You can run the computational graph using the following:</p>\n",
    "<p>```sess.run(fetches, feed_dict=None, options=None, run_metadata=None)```</p>\n",
    "<p>This code take the Tensors you ask for it to return, resolves the graph to include only everything needed to return those Tensors, and then substitutes in the Tensors in the feed_dict into the computational graph and runs it.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#To remember the parameters for evaluation-time\n",
    "w_val, b_val = None, None\n",
    "\n",
    "#Spin up a new session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    #Feed in each example one by one, for 10 epochs\n",
    "    for j in range(10):\n",
    "        #Separate the sqft and the price from the raw data input\n",
    "        sq_feet_normed = [[norm_housing_data[i][0]] for i in range(len(housing_data))]\n",
    "        actual_price_normed = [[norm_housing_data[i][1]] for i in range(len(housing_data))]\n",
    "        \n",
    "        for i in range(len(housing_data)):\n",
    "        #FILL IN TRAINING STEP\n",
    "        \n",
    "        #Randomly shuffle training data\n",
    "        np.random.shuffle(norm_housing_data)\n",
    "        \n",
    "print(w_val)\n",
    "print(b_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Concept 7: TensorFlow Evaluation</h1>\n",
    "<p>Finally, we're ready to see how well our model does. Let's run through and see what the error is on some random examples taken from the same distribution. To do this, we start a new session (isolated from the first for safety). Then, we evaluate the price_pred, giving the feed_dict our extracted values for the weights and biases. For each prediction, we calcualte the error, and then at the end we average over all of the errors.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Error and predictions\n",
    "err = 0\n",
    "preds = []\n",
    "\n",
    "#Spin up a new session\n",
    "with tf.Session() as sess:    \n",
    "    \n",
    "    #Iterate through testing data\n",
    "    for example in norm_housing_test_data:\n",
    "        pred = sess.run(price_pred, feed_dict = {sq_feet: [[example[0]]], \n",
    "                                                                 w: w_val, \n",
    "                                                                 b: b_val})\n",
    "        \n",
    "        #Return predictions and true p back to i/o space and calculate the error\n",
    "        _, pred = None, None #REPLACE LINE\n",
    "        _, true_p = None, None #REPLACE LINE\n",
    "        preds.append(pred)\n",
    "        err += None #REPLACE LINE\n",
    "    \n",
    "print('Total error: ' + str(err / len(preds)))\n",
    "print('Total accuracy: ' + str(1 - err / len(preds)))\n",
    "\n",
    "graph_data = list(zip(*housing_test_data))\n",
    "preds_x = [return_to_io_space(item, 0)[0] for item in norm_housing_test_data[:,0]]\n",
    "plt.scatter(graph_data[0], graph_data[1])\n",
    "plt.scatter(preds_x, preds)\n",
    "plt.xlabel('Square Footage')\n",
    "plt.ylabel('Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Special Techniques: Kernel Trick</h1>\n",
    "<p> A fun thing happens when the data is non-linear. How do you predict data using such a model? Thankfully, the design of gradient-descent based linear regression lends itself to one of the most useful techniques in data science and machine learning, often called the Kernel Trick. Essentially, you design a kernel function mapping some higher-dimensional, potentially nonlinear space to a linear space. For regression, this is not very interesting, mainly because there is very little dimensionality reduction that can actually be implemented. However, later on, this will come in handy.</p> \n",
    "<p> An example of where this can be used is in quadratic regression. Let's use quadratic regression to analyze the same data from above. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#FILL IN CODE FOR COMPUTATION GRAPH AND META GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#FILL IN CODE FOR TRAINING RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Error and predictions\n",
    "err = 0\n",
    "preds = []\n",
    "\n",
    "#FILL IN CODE FOR EVALUATION\n",
    "        \n",
    "print('Total error: ' + str(err / len(preds)))\n",
    "print('Total accuracy: ' + str(1 - err / len(preds)))\n",
    "\n",
    "graph_data = list(zip(*housing_test_data))\n",
    "preds_x = [return_to_io_space(item, 0)[0] for item in norm_housing_test_data[:,0]]\n",
    "plt.scatter(graph_data[0], graph_data[1])\n",
    "plt.scatter(preds_x, preds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Logistical Regression</h1>\n",
    "<p>Logistical Regression is often used for classification. Given some input data, it is reasonable to wonder: is this part of class A or class B? For example, given patient data, one can try to answer the question whether somebody has cancer or not. Let's use our knowledge of computational graphs to generate a model for this.</p>\n",
    "\n",
    "<h2>Concept 8: Activation Functions</h2>\n",
    "<p>Before we do so, however, we need to introduce the concept of an activation function. Activation functions are functions that alter the inputs into them in a nonlinear way and have easily computable derivatives for easy backpropagation. This allows them to learn nonlinear relationships. They can also allow for interpretation of the internals of a neural network. For example, a sigmoid function takes in inputs, scales them to between 0 and 1, and can therefore be interpreted as a probability of a certain class.</p>\n",
    "<p>To add a sigmoid function to a computation graph, simply call: <br><br>\n",
    "```tf.sigmoid(x)```</p>\n",
    "<p>To utilize sigmoid loss, simply call: <br><br>\n",
    "```tf.nn.sigmoid_cross_entropy_with_logits(a, b)```</p>\n",
    "<p>As an exercise now, create a model that takes in 9 inputs and outputs the probability that a certain patient has cancer. Then, create the loss and training steps to train the model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#FILL IN CODE FOR COMPUTATIONAL GRAPH AND META GRAPH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p>Now, let's write the training session and loop.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "patient_data = datasets.cancer_data_gen()\n",
    "patient_data, patient_test_data = patient_data[:550], patient_data[550:]\n",
    "\n",
    "input_patient_data = [patient_data[i][:len(patient_data[i]) - 1] for i in range(len(patient_data))]\n",
    "input_patient_diag = [[1 if patient_data[i][-1] == 4 else 0] for i in range(len(patient_data))]\n",
    "\n",
    "#FILL IN CODE FOR TRAINING RUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, let's evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "acc = 0\n",
    "preds = []\n",
    "\n",
    "#Separate input data from the diagnosis data\n",
    "input_test_data = [patient_test_data[i][:len(patient_test_data[i]) - 1] for i in range(len(patient_test_data))]\n",
    "input_test_diag = [[1 if patient_test_data[i][-1] == 4 else 0] for i in range(len(patient_test_data))]\n",
    "\n",
    "#FILL IN CODE FOR EVALUATION RUN\n",
    "#print(str(pred[0]) + ' ?= ' + str(input_test_diag[i][0])) #UNCOMMENT LATER\n",
    "        \n",
    "print('Accuracy: ' + str(acc / len(preds)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
